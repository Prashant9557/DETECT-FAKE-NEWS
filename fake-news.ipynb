{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e344b66a-b165-48a6-917f-c3e85b398310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.11.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "461d14d5-0a1b-4159-b3e1-eb560a58d0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
      "        num_rows: 24353\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
      "        num_rows: 8117\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
      "        num_rows: 8117\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"GonzaloA/fake_news\")\n",
    "\n",
    "# Display the dataset structure\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b816328d-22b6-474d-a747-195cebbc1976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 0, 'title': ' ‘Maury’ Show Official Facebook Posts F*CKED UP Caption On Guest That Looks Like Ted Cruz (IMAGE)', 'text': 'Maury is perhaps one of the trashiest shows on television today. It s right in line with the likes of the gutter trash that is Jerry Springer, and the fact that those shows are still on the air with the shit they air really is a sad testament to what Americans find to be entertaining. However, Maury really crossed the line with a Facebook post regarding one of their guest s appearance with a vile, disgusting caption on Tuesday evening.There was a young woman on there doing one of their episodes regarding the paternity of her child. However, on the page, the show posted an image of the woman, who happens to bear a striking resemblance to Senator and presidential candidate Ted Cruz. The caption from the Maury Show page read: The Lie Detector Test determined .that was a LIE!  Ted Cruz is just NOT that SEXY! As if that weren t horrible enough, the caption underneath the Imgur upload reads,  Ted Cruz in drag on Maury. Here is an image from the official Maury Facebook page:Here is the embed of the post itself:This is beyond despicable. It s bad enough that this show preys on desperate people to keep their trashy show going and their audience of bottom-feeders entertained, but now they publicly mock them as well? This young woman cannot help how she looks or who she resembles. That is not her fault. Shaming someone s looks on social media is something we d expect from the morons who watch this crap on a daily basis, but it is NOT something the official show page should be doing. Then again, what can you expect from a show that rolls in the mud for a living and continues to show the world that there is now low they will not stoop to? This was more than a step too far, though.Maury, you owe this young woman a public apology. A VERY public apology. There s just no excuse for this, no matter the demographics of your audience or what you do on that disgusting show of yours. I suppose it will be too much to ask that you lose viewers over this, because the people who watch your trashy ass show likely aren t educated enough to understand why this is so wrong in the first place. I don t watch, so I can t deprive you of my viewership, but I CAN call you out.Shame on you, Maury Show and everyone associated with this despicable Facebook post. You really showed your true colors here today.Featured image via Facebook ', 'label': 0}\n",
      "{'Unnamed: 0': [0, 1, 2, 3, 4], 'title': [' ‘Maury’ Show Official Facebook Posts F*CKED UP Caption On Guest That Looks Like Ted Cruz (IMAGE)', ' Trump’s Favorite News Channel Tries To Soothe His Battered Ego – Gets Taken To The Cleaners', 'Russia warns Iraq, Kurds not to destabilize Middle East after Kurdish vote', 'WATCH STEVE SCALISE Throw A Strike At The Nationals Baseball Game [Video]', ' Trump Will HATE What Stephen Colbert Just Did To Him – It’s Pure Comedy Genius (VIDEO)'], 'text': ['Maury is perhaps one of the trashiest shows on television today. It s right in line with the likes of the gutter trash that is Jerry Springer, and the fact that those shows are still on the air with the shit they air really is a sad testament to what Americans find to be entertaining. However, Maury really crossed the line with a Facebook post regarding one of their guest s appearance with a vile, disgusting caption on Tuesday evening.There was a young woman on there doing one of their episodes regarding the paternity of her child. However, on the page, the show posted an image of the woman, who happens to bear a striking resemblance to Senator and presidential candidate Ted Cruz. The caption from the Maury Show page read: The Lie Detector Test determined .that was a LIE!  Ted Cruz is just NOT that SEXY! As if that weren t horrible enough, the caption underneath the Imgur upload reads,  Ted Cruz in drag on Maury. Here is an image from the official Maury Facebook page:Here is the embed of the post itself:This is beyond despicable. It s bad enough that this show preys on desperate people to keep their trashy show going and their audience of bottom-feeders entertained, but now they publicly mock them as well? This young woman cannot help how she looks or who she resembles. That is not her fault. Shaming someone s looks on social media is something we d expect from the morons who watch this crap on a daily basis, but it is NOT something the official show page should be doing. Then again, what can you expect from a show that rolls in the mud for a living and continues to show the world that there is now low they will not stoop to? This was more than a step too far, though.Maury, you owe this young woman a public apology. A VERY public apology. There s just no excuse for this, no matter the demographics of your audience or what you do on that disgusting show of yours. I suppose it will be too much to ask that you lose viewers over this, because the people who watch your trashy ass show likely aren t educated enough to understand why this is so wrong in the first place. I don t watch, so I can t deprive you of my viewership, but I CAN call you out.Shame on you, Maury Show and everyone associated with this despicable Facebook post. You really showed your true colors here today.Featured image via Facebook ', 'Yesterday, after the father of one of the UCLA players arrested in China failed to show Trump proper gratitude for getting his kid released, Trump, predictably, went to Twitter to grouse about it. He seems to expect to be worshiped for his help on this matter, but LaVar Ball wouldn t do it, so Trump tweeted:Now that the three basketball players are out of China and saved from years in jail, LaVar Ball, the father of LiAngelo, is unaccepting of what I did for his son and that shoplifting is no big deal. I should have left them in jail!  Donald J. Trump (@realDonaldTrump) November 19, 2017Fox News put that tweet into a meme portraying Trump as a strong, decisive leader and the UCLA basketball players as weaklings, because of course they did. Then they asked:  Do you agree with President Trump? Yes. Fox News seriously asked people whether they agree that Trump should have left them in jail because the father of one is refusing to show proper gratitude:Do you agree with President @realDonaldTrump? pic.twitter.com/p3VdoXPxPG  Fox News (@FoxNews) November 20, 2017And Twitter is just not having this at all:Nope. He needs adoration- sign of insecurity. That s why he mocks others. Worthless. pic.twitter.com/XpI18s83Wx  Billy Depp (@ucla_007) November 20, 2017I ve never seen a Fox tweet critical of the president.All Fox is does is aid and abet insanity, hate and divisiveness that hurts America. pic.twitter.com/HrsATfLSMI  American Patriot (@RealPatriot1976) November 20, 2017It s all relative. Shoplifting is a crime but its totally eclipsed by Trump University.  Cody Swan (@Lampliighter) November 20, 2017Yeah, POTUS should only do his job if he gets his ass kissed properly. Are you high?  Bill Raudenbush (@CandidateBill) November 20, 2017No. He s a child!  Opinionated (@letmesharewithu) November 20, 2017nah i know what public service means   o  (@geofftype) November 20, 2017I think he should stop being petty af. I don t care to hear about this from the President of the United States. He s so insecure, it s beyond troubling.  Keisha Venezio (@keishhhha) November 20, 2017No, I don t agree  pic.twitter.com/7N9KI0oTxl  Kelly H (@Kellyk1969) November 20, 2017Do you agree with President @realDonaldTrump? pic.twitter.com/p3VdoXPxPG  Fox News (@FoxNews) November 20, 2017Nope. I personally would be extraordinarily appreciative if this happened to one of my kids, but i think it s shameful that the prez resents not being thanked enough and doesn t view helping Americans as just part of the job.  Liberal Kansan (@pvliberal) November 20, 2017NO Disgusting & embarrassing!  Melanie (@Melanie03630436) November 20, 2017Well, I m not convinced that he single-handedly got them out of jail  so the question isn t really valid  Isaac Simonelli (@DiceTravels) November 20, 2017No normal person does. And YOU know it.  Ernie Page (@AngryHatter) November 20, 2017What?! This man is a child in a rumpled suit. In case you haven t noticed, hardly anybody likes him. : Thus, when you give to the needy, sound no trumpet before you, as the hypocrites do in the synagogues and in the streets, that they may be praised by others.  pic.twitter.com/ntJzgkdWdz  DEMOS..We the people (@Windemere22) November 20, 2017China should have thrown #Dotard @realDonaldTrump into the slammer, chucked the key into the bog & flushed it away.  Holger  \\\\_( )_/  (@hnelke1973) November 20, 2017No. @realDonaldTrump has no idea what it means to do good for the sake of doing good. He is an empty shell of a man with no moral compass. Also not my favorite president.  Doreen Graham (@DoreeGraham) November 20, 2017Absolutely not. The fact that our narcissistic baby in chief can t take any criticism without lashing out is disgraceful. We deserve better  erin o (@erin0331) November 20, 2017Typical trump .. always about himself forgetting America is for all  SARA_PDaily (@sarapdaily) November 20, 2017Typical trump .. always about himself forgetting America is for all  SARA_PDaily (@sarapdaily) November 20, 2017Why would anyone agree with this shit? Remember when we mourned the death of the student who died in NK and now we re talking about this?  Aditya Sharma (@adityaksharma) November 20, 2017No I do not agree with Donald. As a matter of fact, I wish he was left in the hospital at the time of his birth.  RABell (@R_A_Bell) November 20, 2017President Man-Baby needs to grow up. And Fox News needs to stop feeding his bullshit.Featured image via Alex Wong/Getty Images', 'MOSCOW (Reuters) - Russia on Wednesday warned Iraq and the Kurds against taking any steps that might destabilize the Middle East after a Kurdish independence referendum, encouraging both sides to hold talks to find a solution within the framework of a single Iraqi state. The Russian Foreign Ministry, in the same statement, also said that while Moscow respected the Kurds  national ambitions it favored preserving the territorial integrity of Iraq. ', \"House Majority Whip Steve Scalise (R., La.) threw a strike in his ceremonial first pitch at the Washington National s opening-round playoff game Friday night, which was also his birthday.Scalise was shot in the hip and nearly died in June and he had to throw from a walker but he had no trouble delivering a perfect pitch to Capitol police special agent David Bailey, who was also injured in the shooting.After taking a bullet on a field in Alexandria, Va., Scalise was visibly happy to return to the diamond, and he expressed his gratefulness on Twitter after.What a memorable birthday! I threw the first pitch at tonight's @Nationals vs. @Cubs #NLDS game! pic.twitter.com/1F7XrRGwiP  Rep. Steve Scalise (@SteveScalise) October 7, 2017Scalise returned to the House of Representatives last week to warm greetings from his colleagues, and he expressed thankfulness for his recovery. It s only strengthened my faith in God, and it s really crystallized what shows up as the goodness in people,  he said.  I got to see that goodness in people, and so while some people might focus on a tragic event and an evil act, to me, all I ll remember are the thousands of acts and kindness and love that came out of this.  Read more: WFB\", 'It can be said that Late Show host Stephen Colbert clearly gives no f*cks about what Donald Trump thinks of him. In addition, if Trump were to decide to angry-tweet Colbert in retaliation, you know that Colbert would wear that Twitter fit as a badge of honor, as he should.Absolutely annihilating Trump, his connection to Putin, the Russian prostitute story, the fact that he wants to take a mini-vacation after the inauguration, and the fact that, well, he s clearly full of sh*t, Colbert didn t hold back even bringing God, yes that God almighty with a gun, into the mix.Colbert even brought up the fact that Trump s approval ratings are absolutely pathetic, especially when compared to President Obama s approval rating upon entering office. Not only that, but Colbert had to admit something terrible   that he and white supremacists have one thing in common, they are absolutely disappointed in Trump   albeit clearly for different reasons.The Late Show host even pointed out that Trump will blame everything on being rigged if it doesn t go his way. Even if the weather is bad on Inauguration Day, that s clearly a conspiracy against him.Watch Colbert absolutely shred Trump here:Featured image via video screen capture'], 'label': [0, 0, 1, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# View the first example in the dataset\n",
    "print(ds[\"train\"][0])\n",
    "\n",
    "# View the first five examples\n",
    "print(ds[\"train\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "21491148-68ac-424f-9175-b2eb1752327a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': [0, 1, 2, 3, 4], 'title': [' ‘Maury’ Show Official Facebook Posts F*CKED UP Caption On Guest That Looks Like Ted Cruz (IMAGE)', ' Trump’s Favorite News Channel Tries To Soothe His Battered Ego – Gets Taken To The Cleaners', 'Russia warns Iraq, Kurds not to destabilize Middle East after Kurdish vote', 'WATCH STEVE SCALISE Throw A Strike At The Nationals Baseball Game [Video]', ' Trump Will HATE What Stephen Colbert Just Did To Him – It’s Pure Comedy Genius (VIDEO)'], 'text': ['Maury is perhaps one of the trashiest shows on television today. It s right in line with the likes of the gutter trash that is Jerry Springer, and the fact that those shows are still on the air with the shit they air really is a sad testament to what Americans find to be entertaining. However, Maury really crossed the line with a Facebook post regarding one of their guest s appearance with a vile, disgusting caption on Tuesday evening.There was a young woman on there doing one of their episodes regarding the paternity of her child. However, on the page, the show posted an image of the woman, who happens to bear a striking resemblance to Senator and presidential candidate Ted Cruz. The caption from the Maury Show page read: The Lie Detector Test determined .that was a LIE!  Ted Cruz is just NOT that SEXY! As if that weren t horrible enough, the caption underneath the Imgur upload reads,  Ted Cruz in drag on Maury. Here is an image from the official Maury Facebook page:Here is the embed of the post itself:This is beyond despicable. It s bad enough that this show preys on desperate people to keep their trashy show going and their audience of bottom-feeders entertained, but now they publicly mock them as well? This young woman cannot help how she looks or who she resembles. That is not her fault. Shaming someone s looks on social media is something we d expect from the morons who watch this crap on a daily basis, but it is NOT something the official show page should be doing. Then again, what can you expect from a show that rolls in the mud for a living and continues to show the world that there is now low they will not stoop to? This was more than a step too far, though.Maury, you owe this young woman a public apology. A VERY public apology. There s just no excuse for this, no matter the demographics of your audience or what you do on that disgusting show of yours. I suppose it will be too much to ask that you lose viewers over this, because the people who watch your trashy ass show likely aren t educated enough to understand why this is so wrong in the first place. I don t watch, so I can t deprive you of my viewership, but I CAN call you out.Shame on you, Maury Show and everyone associated with this despicable Facebook post. You really showed your true colors here today.Featured image via Facebook ', 'Yesterday, after the father of one of the UCLA players arrested in China failed to show Trump proper gratitude for getting his kid released, Trump, predictably, went to Twitter to grouse about it. He seems to expect to be worshiped for his help on this matter, but LaVar Ball wouldn t do it, so Trump tweeted:Now that the three basketball players are out of China and saved from years in jail, LaVar Ball, the father of LiAngelo, is unaccepting of what I did for his son and that shoplifting is no big deal. I should have left them in jail!  Donald J. Trump (@realDonaldTrump) November 19, 2017Fox News put that tweet into a meme portraying Trump as a strong, decisive leader and the UCLA basketball players as weaklings, because of course they did. Then they asked:  Do you agree with President Trump? Yes. Fox News seriously asked people whether they agree that Trump should have left them in jail because the father of one is refusing to show proper gratitude:Do you agree with President @realDonaldTrump? pic.twitter.com/p3VdoXPxPG  Fox News (@FoxNews) November 20, 2017And Twitter is just not having this at all:Nope. He needs adoration- sign of insecurity. That s why he mocks others. Worthless. pic.twitter.com/XpI18s83Wx  Billy Depp (@ucla_007) November 20, 2017I ve never seen a Fox tweet critical of the president.All Fox is does is aid and abet insanity, hate and divisiveness that hurts America. pic.twitter.com/HrsATfLSMI  American Patriot (@RealPatriot1976) November 20, 2017It s all relative. Shoplifting is a crime but its totally eclipsed by Trump University.  Cody Swan (@Lampliighter) November 20, 2017Yeah, POTUS should only do his job if he gets his ass kissed properly. Are you high?  Bill Raudenbush (@CandidateBill) November 20, 2017No. He s a child!  Opinionated (@letmesharewithu) November 20, 2017nah i know what public service means   o  (@geofftype) November 20, 2017I think he should stop being petty af. I don t care to hear about this from the President of the United States. He s so insecure, it s beyond troubling.  Keisha Venezio (@keishhhha) November 20, 2017No, I don t agree  pic.twitter.com/7N9KI0oTxl  Kelly H (@Kellyk1969) November 20, 2017Do you agree with President @realDonaldTrump? pic.twitter.com/p3VdoXPxPG  Fox News (@FoxNews) November 20, 2017Nope. I personally would be extraordinarily appreciative if this happened to one of my kids, but i think it s shameful that the prez resents not being thanked enough and doesn t view helping Americans as just part of the job.  Liberal Kansan (@pvliberal) November 20, 2017NO Disgusting & embarrassing!  Melanie (@Melanie03630436) November 20, 2017Well, I m not convinced that he single-handedly got them out of jail  so the question isn t really valid  Isaac Simonelli (@DiceTravels) November 20, 2017No normal person does. And YOU know it.  Ernie Page (@AngryHatter) November 20, 2017What?! This man is a child in a rumpled suit. In case you haven t noticed, hardly anybody likes him. : Thus, when you give to the needy, sound no trumpet before you, as the hypocrites do in the synagogues and in the streets, that they may be praised by others.  pic.twitter.com/ntJzgkdWdz  DEMOS..We the people (@Windemere22) November 20, 2017China should have thrown #Dotard @realDonaldTrump into the slammer, chucked the key into the bog & flushed it away.  Holger  \\\\_( )_/  (@hnelke1973) November 20, 2017No. @realDonaldTrump has no idea what it means to do good for the sake of doing good. He is an empty shell of a man with no moral compass. Also not my favorite president.  Doreen Graham (@DoreeGraham) November 20, 2017Absolutely not. The fact that our narcissistic baby in chief can t take any criticism without lashing out is disgraceful. We deserve better  erin o (@erin0331) November 20, 2017Typical trump .. always about himself forgetting America is for all  SARA_PDaily (@sarapdaily) November 20, 2017Typical trump .. always about himself forgetting America is for all  SARA_PDaily (@sarapdaily) November 20, 2017Why would anyone agree with this shit? Remember when we mourned the death of the student who died in NK and now we re talking about this?  Aditya Sharma (@adityaksharma) November 20, 2017No I do not agree with Donald. As a matter of fact, I wish he was left in the hospital at the time of his birth.  RABell (@R_A_Bell) November 20, 2017President Man-Baby needs to grow up. And Fox News needs to stop feeding his bullshit.Featured image via Alex Wong/Getty Images', 'MOSCOW (Reuters) - Russia on Wednesday warned Iraq and the Kurds against taking any steps that might destabilize the Middle East after a Kurdish independence referendum, encouraging both sides to hold talks to find a solution within the framework of a single Iraqi state. The Russian Foreign Ministry, in the same statement, also said that while Moscow respected the Kurds  national ambitions it favored preserving the territorial integrity of Iraq. ', \"House Majority Whip Steve Scalise (R., La.) threw a strike in his ceremonial first pitch at the Washington National s opening-round playoff game Friday night, which was also his birthday.Scalise was shot in the hip and nearly died in June and he had to throw from a walker but he had no trouble delivering a perfect pitch to Capitol police special agent David Bailey, who was also injured in the shooting.After taking a bullet on a field in Alexandria, Va., Scalise was visibly happy to return to the diamond, and he expressed his gratefulness on Twitter after.What a memorable birthday! I threw the first pitch at tonight's @Nationals vs. @Cubs #NLDS game! pic.twitter.com/1F7XrRGwiP  Rep. Steve Scalise (@SteveScalise) October 7, 2017Scalise returned to the House of Representatives last week to warm greetings from his colleagues, and he expressed thankfulness for his recovery. It s only strengthened my faith in God, and it s really crystallized what shows up as the goodness in people,  he said.  I got to see that goodness in people, and so while some people might focus on a tragic event and an evil act, to me, all I ll remember are the thousands of acts and kindness and love that came out of this.  Read more: WFB\", 'It can be said that Late Show host Stephen Colbert clearly gives no f*cks about what Donald Trump thinks of him. In addition, if Trump were to decide to angry-tweet Colbert in retaliation, you know that Colbert would wear that Twitter fit as a badge of honor, as he should.Absolutely annihilating Trump, his connection to Putin, the Russian prostitute story, the fact that he wants to take a mini-vacation after the inauguration, and the fact that, well, he s clearly full of sh*t, Colbert didn t hold back even bringing God, yes that God almighty with a gun, into the mix.Colbert even brought up the fact that Trump s approval ratings are absolutely pathetic, especially when compared to President Obama s approval rating upon entering office. Not only that, but Colbert had to admit something terrible   that he and white supremacists have one thing in common, they are absolutely disappointed in Trump   albeit clearly for different reasons.The Late Show host even pointed out that Trump will blame everything on being rigged if it doesn t go his way. Even if the weather is bad on Inauguration Day, that s clearly a conspiracy against him.Watch Colbert absolutely shred Trump here:Featured image via video screen capture'], 'label': [0, 0, 1, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"train\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "23994b99-6665-49d4-beb6-f6ae2e0b4f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
      "    num_rows: 0\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(ds[\"train\"].filter(lambda x: x[\"text\"] is None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0f03d8a6-15cd-48a4-8f20-eec0c7dffa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.filter(lambda x: x[\"text\"] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "872db27f-67c7-4f34-8b1e-ddd474effa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(example):\n",
    "    example[\"text\"] = example[\"text\"].lower()\n",
    "    return example\n",
    "\n",
    "ds = ds.map(to_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bf094d32-48a1-4513-9dac-788a880bd903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(example):\n",
    "    example[\"text\"] = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", example[\"text\"])\n",
    "    return example\n",
    "\n",
    "ds = ds.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fa0dfdbb-f622-40d6-813c-8f39d95b1dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e5e0330b-4ff8-470b-84c0-a93a6a52c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_data(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "ds = ds.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cc1ba16c-317c-4cd6-9fa3-18f7008342c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e0ccaaf4-26fc-4943-8b22-dd512c48571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9698213919113119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      2261\n",
      "           1       0.97      0.97      0.97      2610\n",
      "\n",
      "    accuracy                           0.97      4871\n",
      "   macro avg       0.97      0.97      0.97      4871\n",
      "weighted avg       0.97      0.97      0.97      4871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset and split\n",
    "ds = load_dataset(\"GonzaloA/fake_news\")\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform([x[\"text\"] for x in ds[\"train\"]])\n",
    "X_test = vectorizer.transform([x[\"text\"] for x in ds[\"test\"]])\n",
    "\n",
    "# Extract labels\n",
    "y_train = [x[\"label\"] for x in ds[\"train\"]]\n",
    "y_test = [x[\"label\"] for x in ds[\"test\"]]\n",
    "\n",
    "# Train logistic regression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7e78d3af-d57b-425b-b220-a031db25c4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9802915212482036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      2310\n",
      "           1       0.99      0.97      0.98      2561\n",
      "\n",
      "    accuracy                           0.98      4871\n",
      "   macro avg       0.98      0.98      0.98      4871\n",
      "weighted avg       0.98      0.98      0.98      4871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset and split\n",
    "ds = load_dataset(\"GonzaloA/fake_news\")\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform([x[\"text\"] for x in ds[\"train\"]])\n",
    "X_test = vectorizer.transform([x[\"text\"] for x in ds[\"test\"]])\n",
    "\n",
    "# Extract labels\n",
    "y_train = [x[\"label\"] for x in ds[\"train\"]]\n",
    "y_test = [x[\"label\"] for x in ds[\"test\"]]\n",
    "\n",
    "# Initialize and train the Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = gb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "60ec97e7-22bd-4e23-867d-5d540de6af1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9784438513652227\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      2221\n",
      "           1       0.99      0.97      0.98      2650\n",
      "\n",
      "    accuracy                           0.98      4871\n",
      "   macro avg       0.98      0.98      0.98      4871\n",
      "weighted avg       0.98      0.98      0.98      4871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset and split\n",
    "ds = load_dataset(\"GonzaloA/fake_news\")\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform([x[\"text\"] for x in ds[\"train\"]])\n",
    "X_test = vectorizer.transform([x[\"text\"] for x in ds[\"test\"]])\n",
    "\n",
    "# Extract labels\n",
    "y_train = [x[\"label\"] for x in ds[\"train\"]]\n",
    "y_test = [x[\"label\"] for x in ds[\"test\"]]\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4915baa9-d82f-40b0-a31f-14da1a010ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9189078218025046\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91      2253\n",
      "           1       0.93      0.92      0.92      2618\n",
      "\n",
      "    accuracy                           0.92      4871\n",
      "   macro avg       0.92      0.92      0.92      4871\n",
      "weighted avg       0.92      0.92      0.92      4871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset and split\n",
    "ds = load_dataset(\"GonzaloA/fake_news\")\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform([x[\"text\"] for x in ds[\"train\"]])\n",
    "X_test = vectorizer.transform([x[\"text\"] for x in ds[\"test\"]])\n",
    "\n",
    "# Extract labels\n",
    "y_train = [x[\"label\"] for x in ds[\"train\"]]\n",
    "y_test = [x[\"label\"] for x in ds[\"test\"]]\n",
    "\n",
    "# Initialize and train the Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = nb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e96a4364-d555-4d2d-8d5d-454ab8e100f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.9692\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      2258\n",
      "           1       0.97      0.97      0.97      2613\n",
      "\n",
      "    accuracy                           0.97      4871\n",
      "   macro avg       0.97      0.97      0.97      4871\n",
      "weighted avg       0.97      0.97      0.97      4871\n",
      "\n",
      "==================================================\n",
      "Model: Gradient Boosting\n",
      "Accuracy: 0.9801\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      2258\n",
      "           1       0.99      0.97      0.98      2613\n",
      "\n",
      "    accuracy                           0.98      4871\n",
      "   macro avg       0.98      0.98      0.98      4871\n",
      "weighted avg       0.98      0.98      0.98      4871\n",
      "\n",
      "==================================================\n",
      "Model: Random Forest\n",
      "Accuracy: 0.9817\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      2258\n",
      "           1       0.99      0.97      0.98      2613\n",
      "\n",
      "    accuracy                           0.98      4871\n",
      "   macro avg       0.98      0.98      0.98      4871\n",
      "weighted avg       0.98      0.98      0.98      4871\n",
      "\n",
      "==================================================\n",
      "Model: Naive Bayes\n",
      "Accuracy: 0.9179\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91      2258\n",
      "           1       0.92      0.92      0.92      2613\n",
      "\n",
      "    accuracy                           0.92      4871\n",
      "   macro avg       0.92      0.92      0.92      4871\n",
      "weighted avg       0.92      0.92      0.92      4871\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"GonzaloA/fake_news\")\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform([x[\"text\"] for x in ds[\"train\"]])\n",
    "X_test = vectorizer.transform([x[\"text\"] for x in ds[\"test\"]])\n",
    "\n",
    "# Extract labels\n",
    "y_train = [x[\"label\"] for x in ds[\"train\"]]\n",
    "y_test = [x[\"label\"] for x in ds[\"test\"]]\n",
    "\n",
    "# Initialize each model\n",
    "log_reg_model = LogisticRegression(max_iter=1000)\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train all models on the training data\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# List of models for evaluation\n",
    "models = {\n",
    "    \"Logistic Regression\": log_reg_model,\n",
    "    \"Gradient Boosting\": gb_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Naive Bayes\": nb_model\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print model's name, accuracy, and classification report\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ad6e25fc-aee8-4af1-9763-14a113566e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    " new_texts = [\n",
    "     \"Why is Elon Musk becoming Donald Trump's efficiency tsar Billionaire Elon Musk has been tasked with leading incoming President Donald Trump's new Department of Government Efficiency (Doge) In a statement on social media, the US president-elect said Musk - along with former Republican presidential candidate Vivek Ramaswamy - would dismantle government bureaucracy, slash excess regulations, cut wasteful expenditures and restructure federal agencies It is a role that the tech entrepreneur has arguably prepared for through his business leadership and one he has spent months arguing for But it is also one that is expected to garner him influence over government policy and the regulatory environment his enterprises exist in\"\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "99edd0cc-e3b7-4b1e-b969-d7cf727dc2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts_vectorized = vectorizer.transform(new_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "77916f14-4a7a-44d1-bd98-72401f99b087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Text 1: Real\n",
      "==================================================\n",
      "Model: Gradient Boosting\n",
      "Text 1: Fake\n",
      "==================================================\n",
      "Model: Random Forest\n",
      "Text 1: Fake\n",
      "==================================================\n",
      "Model: Naive Bayes\n",
      "Text 1: Real\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Define a function to make predictions for the new texts\n",
    "def check_fake_news(models, new_texts_vectorized):\n",
    "    for model_name, model in models.items():\n",
    "        y_pred = model.predict(new_texts_vectorized)\n",
    "        \n",
    "        # Print out the predictions\n",
    "        print(f\"Model: {model_name}\")\n",
    "        for i, text in enumerate(new_texts):\n",
    "            print(f\"Text {i+1}: {'Fake' if y_pred[i] == 0 else 'Real'}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Check predictions (make sure this line is at the correct indentation level)\n",
    "check_fake_news(models, new_texts_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8232e55b-ec23-442b-80b9-203d63fb0983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Article Text:\n",
      " Every year, India’s salaried class eagerly awaits relief in income tax. This year, the middle class is also penning all their hopes for FM Nirmala Sitharaman, when she presents Budget 2025 on February 1. According to a Reuters report, the government is actively considering tax relief for the middle class, particularly earning up to ₹15 lakh annually. The 2025 Budget, to be tabled on February 1, may include an announcement on the tax breaks. This action is intended to boost consumer spending and stimulate the economy. Livemint couldn't independently verify the information.\n",
      "\n",
      "Income tax budget: Who will benefit? If this is implemented, millions of taxpayers might benefit greatly, especially city dwellers struggling with high living costs. According to the Reuters report, those choosing the 2020 tax scheme, which excludes exclusions like those for housing rents, would be eligible for this.\n",
      "\n",
      "Also Read | How much cash can you receive in your bank savings account in a financial year?\n",
      "\n",
      "Two tax options for income taxpayers Two options are now available to Indian taxpayers: the Old and New Tax Regime. Under the Old Tax Regime, rent and insurance exemptions are allowed, among other expenses. The New Tax Regime lowers tax rates while doing away with most exemptions. The choice that best fits their financial situation is up to the taxpayers to choose.\n",
      "\n",
      "New income tax regime Individuals earning up to ₹3 lakh annually are exempt from income tax under the new tax regime that was implemented in 2020. Income between ₹3 and 7 lakh will be subject to a 5% tax, income between ₹7 and 10 lakh to a 10% tax, and income between ₹10 and 12 lakh to a 15% tax. Furthermore, income between ₹12 and ₹15 lakh would be subject to a 20% tax, while income over ₹15 lakh will be subject to a 30% tax.\n",
      "\n",
      "Slabs Tax rate Upto ₹ 3 lakh Nil ₹ 3-7 lakh 5% ₹ 7-10 lakh 10% ₹ 10-12 lakh 15% ₹ 12-15 lakh 20% Above ₹ 15 lakh 30%\n",
      "\n",
      "Up to ₹3 lakh - Nil\n",
      "\n",
      "₹3-7 lakh - 5%\n",
      "\n",
      "₹7-10 lakh - 10%\n",
      "\n",
      "₹10-12 lakh - 15%\n",
      "\n",
      "12-15 lakh - 20%\n",
      "\n",
      "Above ₹15 lakh - 30%\n",
      "\n",
      "Old income tax regime Under the Old Tax Regime, income up to ₹2.5 lakh is exempt from taxation. For income between ₹2.5 lakh and ₹5 lakh, a tax rate of 5% is applied. Personal income ranging from ₹5 lakh to ₹10 lakh is taxed at 20%, while income exceeding ₹10 lakh is taxed at the highest rate of 30%. This structure allows taxpayers to benefit from various exemptions, including deductions for expenses such as house rent and insurance premiums, providing more flexibility based on individual financial situations.\n",
      "\n",
      "Upto ₹2.5 lakh- Nil\n",
      "\n",
      "₹2.5- ₹5 lakh- 5%.\n",
      "\n",
      "₹5 lakh to ₹10 lakh- 20%.\n",
      "\n",
      "Income above ₹10 lakh- 30%.\n",
      "\n",
      "Slabs Tax rate Upto 2.5 Lakh Nil ₹ 2.5- ₹ 5 lakh 5% ₹ 5- ₹ 10 lakh 20% Above ₹ 10 lakh 30%\n",
      "\n",
      "According to a government source cited in the Reuters report, lowering the tax rate would encourage more individuals to opt for the new, more straightforward tax regime. The report also highlighted that a significant portion of India's current income tax revenue comes from individuals earning at least ₹10 lakh. Under the old tax regime, these earners are taxed at 20%, while the new tax regime imposes a 10% tax rate.\n",
      "\n",
      "Siddharth Maurya, Founder & Managing Director of Vibhavangal Anukulakara Private Limited, suggests that cutting personal income tax for individuals earning up to ₹1.5 million annually could stimulate urban middle-class consumption, which has been affected by inflation and rising living costs.\n",
      "\n",
      "He added that the consideration of this proposal reflects the view of government officials as it relates to middle-class issues, which also revolve around the need and desire to start a consumption-based growth model.\n",
      "\n",
      "Also Read | The most underrated SIP hack for a bigger mutual fund corpus\n",
      "\n",
      "Budget 2025 Finance Minister Nirmala Sitharaman is expected to table the Union Budget 2025 on February 1, 2025. The July presentation of Budget 2024 included many modifications to the income tax laws.\n",
      "\n",
      "Read all our personal finance stories here\n",
      "Model: Logistic Regression\n",
      "Prediction: Real\n",
      "==================================================\n",
      "Model: Gradient Boosting\n",
      "Prediction: Real\n",
      "==================================================\n",
      "Model: Random Forest\n",
      "Prediction: Real\n",
      "==================================================\n",
      "Model: Naive Bayes\n",
      "Prediction: Real\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define function to extract article text from a URL\n",
    "def extract_article(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text\n",
    "\n",
    "# Example usage: Extract article text\n",
    "url = 'https://www.livemint.com/money/personal-finance/income-tax-will-fm-nirmala-sitharaman-announce-tax-relief-for-taxpayers-earning-up-to-15-lakh-in-budget-2025-itr-filing-11735360922393.html'\n",
    "article_text = extract_article(url)\n",
    "\n",
    "# Print the extracted text\n",
    "print(\"Extracted Article Text:\\n\", article_text)\n",
    "\n",
    "# Assuming 'vectorizer' is already trained with your dataset\n",
    "# Vectorize the new article text\n",
    "new_text_vectorized = vectorizer.transform([article_text])\n",
    "\n",
    "# Function to check if the article is fake or real\n",
    "def check_fake_news(models, new_text_vectorized):\n",
    "    for model_name, model in models.items():\n",
    "        y_pred = model.predict(new_text_vectorized)\n",
    "        \n",
    "        # Print the prediction for each model\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Prediction: {'Fake' if y_pred[0] == 0 else 'Real'}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Run the fake news check\n",
    "check_fake_news(models, new_text_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f5ebd673-7779-4b4c-b6ef-1ec94d7b8513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Article Text:\n",
      " Every year, India’s salaried class eagerly awaits relief in income tax. This year, the middle class is also penning all their hopes for FM Nirmala Sitharaman, when she presents Budget 2025 on February 1. According to a Reuters report, the government is actively considering tax relief for the middle class, particularly earning up to ₹15 lakh annually. The 2025 Budget, to be tabled on February 1, may include an announcement on the tax breaks. This action is intended to boost consumer spending and stimulate the economy. Livemint couldn't independently verify the information.\n",
      "\n",
      "Income tax budget: Who will benefit? If this is implemented, millions of taxpayers might benefit greatly, especially city dwellers struggling with high living costs. According to the Reuters report, those choosing the 2020 tax scheme, which excludes exclusions like those for housing rents, would be eligible for this.\n",
      "\n",
      "Also Read | How much cash can you receive in your bank savings account in a financial year?\n",
      "\n",
      "Two tax options for income taxpayers Two options are now available to Indian taxpayers: the Old and New Tax Regime. Under the Old Tax Regime, rent and insurance exemptions are allowed, among other expenses. The New Tax Regime lowers tax rates while doing away with most exemptions. The choice that best fits their financial situation is up to the taxpayers to choose.\n",
      "\n",
      "New income tax regime Individuals earning up to ₹3 lakh annually are exempt from income tax under the new tax regime that was implemented in 2020. Income between ₹3 and 7 lakh will be subject to a 5% tax, income between ₹7 and 10 lakh to a 10% tax, and income between ₹10 and 12 lakh to a 15% tax. Furthermore, income between ₹12 and ₹15 lakh would be subject to a 20% tax, while income over ₹15 lakh will be subject to a 30% tax.\n",
      "\n",
      "Slabs Tax rate Upto ₹ 3 lakh Nil ₹ 3-7 lakh 5% ₹ 7-10 lakh 10% ₹ 10-12 lakh 15% ₹ 12-15 lakh 20% Above ₹ 15 lakh 30%\n",
      "\n",
      "Up to ₹3 lakh - Nil\n",
      "\n",
      "₹3-7 lakh - 5%\n",
      "\n",
      "₹7-10 lakh - 10%\n",
      "\n",
      "₹10-12 lakh - 15%\n",
      "\n",
      "12-15 lakh - 20%\n",
      "\n",
      "Above ₹15 lakh - 30%\n",
      "\n",
      "Old income tax regime Under the Old Tax Regime, income up to ₹2.5 lakh is exempt from taxation. For income between ₹2.5 lakh and ₹5 lakh, a tax rate of 5% is applied. Personal income ranging from ₹5 lakh to ₹10 lakh is taxed at 20%, while income exceeding ₹10 lakh is taxed at the highest rate of 30%. This structure allows taxpayers to benefit from various exemptions, including deductions for expenses such as house rent and insurance premiums, providing more flexibility based on individual financial situations.\n",
      "\n",
      "Upto ₹2.5 lakh- Nil\n",
      "\n",
      "₹2.5- ₹5 lakh- 5%.\n",
      "\n",
      "₹5 lakh to ₹10 lakh- 20%.\n",
      "\n",
      "Income above ₹10 lakh- 30%.\n",
      "\n",
      "Slabs Tax rate Upto 2.5 Lakh Nil ₹ 2.5- ₹ 5 lakh 5% ₹ 5- ₹ 10 lakh 20% Above ₹ 10 lakh 30%\n",
      "\n",
      "According to a government source cited in the Reuters report, lowering the tax rate would encourage more individuals to opt for the new, more straightforward tax regime. The report also highlighted that a significant portion of India's current income tax revenue comes from individuals earning at least ₹10 lakh. Under the old tax regime, these earners are taxed at 20%, while the new tax regime imposes a 10% tax rate.\n",
      "\n",
      "Siddharth Maurya, Founder & Managing Director of Vibhavangal Anukulakara Private Limited, suggests that cutting personal income tax for individuals earning up to ₹1.5 million annually could stimulate urban middle-class consumption, which has been affected by inflation and rising living costs.\n",
      "\n",
      "He added that the consideration of this proposal reflects the view of government officials as it relates to middle-class issues, which also revolve around the need and desire to start a consumption-based growth model.\n",
      "\n",
      "Also Read | The most underrated SIP hack for a bigger mutual fund corpus\n",
      "\n",
      "Budget 2025 Finance Minister Nirmala Sitharaman is expected to table the Union Budget 2025 on February 1, 2025. The July presentation of Budget 2024 included many modifications to the income tax laws.\n",
      "\n",
      "Read all our personal finance stories here\n",
      "Final Prediction: Real\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from newspaper import Article\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define function to extract article text from a URL\n",
    "def extract_article(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text\n",
    "\n",
    "# Example usage: Extract article text from the provided URL\n",
    "url = 'https://www.livemint.com/money/personal-finance/income-tax-will-fm-nirmala-sitharaman-announce-tax-relief-for-taxpayers-earning-up-to-15-lakh-in-budget-2025-itr-filing-11735360922393.html'  # Example article URL\n",
    "article_text = extract_article(url)\n",
    "\n",
    "# Print the extracted text\n",
    "print(\"Extracted Article Text:\\n\", article_text)\n",
    "\n",
    "# Assuming 'vectorizer' is already trained with your dataset\n",
    "# Vectorize the new article text using the fitted vectorizer\n",
    "new_text_vectorized = vectorizer.transform([article_text])\n",
    "\n",
    "# Define the model accuracies (these should match the models you're using)\n",
    "model_accuracies = [0.9717, 0.9811, 0.9780, 0.9230]  # Logistic Regression, Gradient Boosting, Random Forest, Naive Bayes\n",
    "\n",
    "# Function to check if the article is fake or real with weighted decision\n",
    "def check_fake_news_weighted(models, new_text_vectorized, model_accuracies):\n",
    "    model_predictions = []\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    for i, (model_name, model) in enumerate(models.items()):\n",
    "        y_pred = model.predict(new_text_vectorized)\n",
    "        model_predictions.append(y_pred[0])  # Store the prediction of the first (and only) text\n",
    "\n",
    "    # Initialize weighted sum of predictions\n",
    "    weighted_sum = 0\n",
    "    \n",
    "    # Calculate the weighted sum of the predictions based on accuracies\n",
    "    for i, prediction in enumerate(model_predictions):\n",
    "        weighted_sum += prediction * model_accuracies[i]  # Multiply each model's prediction by its accuracy (weight)\n",
    "\n",
    "    # Final decision: sum of weighted predictions\n",
    "    if weighted_sum > 0:\n",
    "        final_prediction = 1  # Consider \"Real\" if weighted sum is positive\n",
    "    else:\n",
    "        final_prediction = 0  # Consider \"Fake\" if weighted sum is zero or negative\n",
    "    \n",
    "    return 'Real' if final_prediction == 1 else 'Fake'\n",
    "\n",
    "# Example models (These need to be defined or loaded based on your previous code)\n",
    "# Assuming you already have these models loaded and trained:\n",
    "models = {\n",
    "    \"Logistic Regression\": log_reg_model,  # Your trained Logistic Regression model\n",
    "    \"Gradient Boosting\": gb_model,        # Your trained Gradient Boosting model\n",
    "    \"Random Forest\": rf_model,            # Your trained Random Forest model\n",
    "    \"Naive Bayes\": nb_model               # Your trained Naive Bayes model\n",
    "}\n",
    "\n",
    "# Run the fake news check with weighted decision\n",
    "final_result = check_fake_news_weighted(models, new_text_vectorized, model_accuracies)\n",
    "print(f\"Final Prediction: {final_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98777c35-671c-4a91-bba8-d49ccb8f16e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Article Text:\n",
      " Twenty years on: 'My boat was metres from the shore when the tsunami hit'\n",
      "\n",
      "BBC Just as we pulled out from the harbour, our boat lurched and suddenly the jetty next to where we had boarded collapsed into the sea\n",
      "\n",
      "Boxing Day, 2004. When the earthquake struck at 06:30 (01:00 GMT), I was on a ferry, headed towards Havelock – an island in the Indian archipelago of Andaman and Nicobar. Known for its silver sand and clear blue waters, the Radhanagar beach there had recently been crowned \"Asia's Best Beach\" by Time magazine. My best friend from college and her family had lived in Port Blair, the capital of the archipelago, for a decade and a half, but this was my first visit to the islands, where I had arrived on Christmas Eve. We had planned to spend three days in Havelock and in the morning we packed snacks and sandwiches, gathered excited children and headed out to catch the ferry from Phoenix Bay jetty in Port Blair. Not wanting to miss out on anything, I was standing on the front deck, looking around, when disaster struck. Just as we pulled out from the harbour, the boat lurched and suddenly the jetty next to where we had boarded crumpled and fell into the sea. It was followed by the watchtower and an electricity pole. It was an extraordinary sight. Dozens of people standing alongside me watched open-mouthed. Thankfully, the jetty was deserted at the time so there were no casualties. A boat was due to leave from there in half an hour but the travellers were yet to arrive.\n",
      "\n",
      "Getty Images The tsunami inundated a large number of homes in low-lying areas\n",
      "\n",
      "A member of the boat's crew told me it was an earthquake. At the time I didn't know, but the 9.1 magnitude quake was the third most powerful ever recorded in the world - and remains the biggest and most destructive in Asia. Occurring off the coast of northwest Sumatra under the Indian Ocean, it unleashed a devastating tsunami that killed an estimated 228,000 people across more than a dozen countries and caused massive damage in Indonesia, Sri Lanka, India, Maldives and Thailand. The Andaman and Nicobar islands, located just about 100km north of the epicentre, suffered extensive damage when a wall of water, as high as 15 metres (49 ft) in places, hit land just about 15 minutes later. The official death toll was put at 1,310 – but with more than 5,600 people missing and presumed dead, it's believed that more than 7,000 islanders perished. While on the boat, however, we were oblivious to the scale of destruction around us. Our mobile phones didn't work on the water and we only got snippets of information from the crew. We heard about damage in Sri Lanka, Thailand and Maldives – and the southern Indian coastal town of Nagapattinam.\n",
      "\n",
      "AFP Indian men stand exhausted after searching for missing relatives in Cuddalore, 27 December 2004\n",
      "\n",
      "But there was no information about Andaman and Nicobar - a collection of hundreds of islands scattered around in the Bay of Bengal, located about 1,500km (915 miles) east of India's mainland. Only 38 of them were inhabited. They were home to 400,000 people, including six hunter-gatherer groups who had lived isolated from the outside world for thousands of years. The only way to get to the islands was by ferries but, as we later learnt, an estimated 94% of the jetties in the region were damaged. That was also the reason why, on 26 December 2004, we never made it to Havelock. The jetty there was damaged and under water, we were told. Watch Geeta Pandey speak about her experience here So the boat turned around and started on its return journey. For a while, there was speculation that we might not get clearance to dock at Port Blair for safety reasons and might have to spend the night at anchor. This made the passengers – most of them tourists looking forward to sun and sand – anxious.\n",
      "\n",
      "After several hours of bobbing along in rough seas, we returned to Port Blair. Because Phoenix Bay had been closed following the morning's damage, we were taken to Chatham, another harbour in Port Blair. The jetty where we were dropped had huge, gaping holes in places. The signs of devastation were all around us as we headed home – buildings had turned into rubble, small upturned boats sat in the middle of the streets and roads had great gashes in them. Thousands of people had been turned homeless when the tidal wave flooded their homes in low-lying areas. I met a traumatised nine-year-old girl whose house was filled with water and she told me she had nearly drowned. A woman told me she had lost her entire life's possessions in the blink of an eye.\n",
      "\n",
      "Getty Images In Port Blair, buildings had turned into rubble, small upturned boats sat in the middle of the streets and roads had huge gashes\n",
      "\n",
      "Over the next three weeks, I reported extensively on the disaster and its effects on the population. It was the first time a tsunami had wreaked such havoc in the Andaman and Nicobar Islands and the scale of the tragedy was overwhelming. Salt water contaminated many sources of fresh water and destroyed large tracts of arable land. Getting vital supplies into the islands was tough with jetties unserviceable. The authorities mounted a huge relief and rescue effort. The army, navy and air force were deployed, but it took days before they could get to all the islands. Every day, navy and coast guard ships brought boatloads of people made homeless by the tsunami from other islands to Port Blair where schools and government buildings were turned into temporary shelters. They brought stories of devastation in their homelands. Many told me they had escaped with nothing but the clothes on their backs. One woman from Car Nicobar told me that when the earthquake struck, the ground started to spew foamy water at the same time as the waves came in from the sea. She and hundreds of others from her village had waited for rescuers without food or water for 48 hours. She said it was a \"miracle\" that she and her 20-day-old baby had survived. Port Blair was almost daily jolted by aftershocks, some of them strong enough to start rumours of fresh tsunamis, making scared people run to get to higher ground.\n",
      "\n",
      "Getty Images Thousands of people were left homeless\n",
      "\n",
      "A few days later, the Indian military flew journalists to Car Nicobar, a flat fertile island known for its enchanting beaches and also home to a large Indian air force colony. The killer tsunami had completely flattened the base. The water rose by 12 metres here and as most people slept, the ground was pulled away from under their feet. A hundred people died here. More than half were air force officers and their families. We visited Malacca and Kaakan villages on the island which also bore the brunt of nature's fury, forcing residents to take shelter in tents along the road. Among them were families torn apart by the tidal wave. A grief-stricken young couple told me they had managed to save their five-month-old baby, but their other children, aged seven and 12, were washed away. Surrounded by coconut palms on all sides, every house had turned into rubble. Among the personal belongings strewn about were clothes, textbooks, a child's shoe and a music keyboard. The only thing that stood - surprisingly intact - was a bust of the father of the Indian nation, Mahatma Gandhi, at a traffic roundabout.\n",
      "\n",
      "Getty Images The Indian air force base in Car Nicobar was flattened by the tidal wave\n",
      "Final Prediction: Real\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from newspaper import Article\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# Define function to extract article text from a URL\n",
    "def extract_article(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text\n",
    "\n",
    "# Example usage: Extract article text from the provided URL\n",
    "url = 'https://www.bbc.com/news/articles/c6230646435o'  # Example article URL\n",
    "article_text = extract_article(url)\n",
    "\n",
    "# Print the extracted text\n",
    "print(\"Extracted Article Text:\\n\", article_text)\n",
    "\n",
    "# Training vectorizer (ensure you use your dataset for fitting)\n",
    "# Assuming 'train_data' is a dictionary containing the keys 'text' and 'label'\n",
    "train_data = {\n",
    "    'text': [\"sample text 1\", \"sample text 2\", \"sample text 3\", \"sample text 4\"],  # Replace with your actual training data\n",
    "    'label': [1, 0, 1, 0]  # Replace with your actual training labels\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "y_train = np.array(train_data['label'])\n",
    "\n",
    "# Train models\n",
    "log_reg_model = LogisticRegression()\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Verify all models are fitted\n",
    "models = {\n",
    "    \"Logistic Regression\": log_reg_model,\n",
    "    \"Gradient Boosting\": gb_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Naive Bayes\": nb_model\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    try:\n",
    "        check_is_fitted(model)\n",
    "    except NotFittedError:\n",
    "        raise ValueError(f\"{model_name} is not fitted. Train it before use.\")\n",
    "\n",
    "# Vectorize the new article text using the fitted vectorizer\n",
    "new_text_vectorized = vectorizer.transform([article_text])\n",
    "\n",
    "# Define the model accuracies (these should match the models you're using)\n",
    "model_accuracies = [0.9717, 0.9811, 0.9780, 0.9230]  # Logistic Regression, Gradient Boosting, Random Forest, Naive Bayes\n",
    "\n",
    "# Function to check if the article is fake or real with weighted decision\n",
    "def check_fake_news_weighted(models, new_text_vectorized, model_accuracies):\n",
    "    model_predictions = []\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    for i, (model_name, model) in enumerate(models.items()):\n",
    "        y_pred = model.predict(new_text_vectorized)\n",
    "        model_predictions.append(y_pred[0])  # Store the prediction of the first (and only) text\n",
    "\n",
    "    # Initialize weighted sum of predictions\n",
    "    weighted_sum = 0\n",
    "    \n",
    "    # Calculate the weighted sum of the predictions based on accuracies\n",
    "    for i, prediction in enumerate(model_predictions):\n",
    "        weighted_sum += prediction * model_accuracies[i]  # Multiply each model's prediction by its accuracy (weight)\n",
    "\n",
    "    # Final decision: sum of weighted predictions\n",
    "    if weighted_sum > 0:\n",
    "        final_prediction = 1  # Consider \"Real\" if weighted sum is positive\n",
    "    else:\n",
    "        final_prediction = 0  # Consider \"Fake\" if weighted sum is zero or negative\n",
    "    \n",
    "    return 'Real' if final_prediction == 1 else 'Fake'\n",
    "\n",
    "# Run the fake news check with weighted decision\n",
    "final_result = check_fake_news_weighted(models, new_text_vectorized, model_accuracies)\n",
    "print(f\"Final Prediction: {final_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7ee3f3d2-2c41-46b5-8112-93db7e2fed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2477265f-954e-4597-8a23-d632fc7378e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer and model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save vectorizer and model\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(model, \"log_reg_model.pkl\")\n",
    "joblib.dump(model, \"rf_model.pkl\")\n",
    "joblib.dump(model, \"gb_model.pkl\")\n",
    "joblib.dump(model, \"nb_model.pkl\")\n",
    "print(\"Vectorizer and model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8009374a-04a0-4003-bb7b-068400544ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Vectorizer vocabulary size (5) does not match model input size (2). Ensure the vectorizer and models are from the same training pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Ensure compatibility\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vectorizer_vocab_size \u001b[38;5;241m!=\u001b[39m model_input_size:\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorizer vocabulary size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvectorizer_vocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match model input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_input_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsure the vectorizer and models are from the same training pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Vectorizer vocabulary size (5) does not match model input size (2). Ensure the vectorizer and models are from the same training pipeline."
     ]
    }
   ],
   "source": [
    "# Verify compatibility between vectorizer and model\n",
    "vectorizer_vocab_size = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Check model type and retrieve input size\n",
    "if isinstance(log_reg_model, LogisticRegression):\n",
    "    model_input_size = log_reg_model.coef_.shape[1]\n",
    "elif isinstance(log_reg_model, MultinomialNB):\n",
    "    model_input_size = log_reg_model.feature_log_prob_.shape[1]\n",
    "else:\n",
    "    raise TypeError(f\"Unsupported model type: {type(log_reg_model)}\")\n",
    "\n",
    "# Ensure compatibility\n",
    "if vectorizer_vocab_size != model_input_size:\n",
    "    raise ValueError(\n",
    "        f\"Vectorizer vocabulary size ({vectorizer_vocab_size}) does not match model input size ({model_input_size}). \"\n",
    "        \"Ensure the vectorizer and models are from the same training pipeline.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "41242b9c-96a4-42a2-b3d2-1adbcd699bf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultinomialNB' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Verify compatibility\u001b[39;00m\n\u001b[0;32m      6\u001b[0m vectorizer_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[1;32m----> 7\u001b[0m log_reg_input_size \u001b[38;5;241m=\u001b[39m \u001b[43mlog_reg_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vectorizer_vocab_size \u001b[38;5;241m!=\u001b[39m log_reg_input_size:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorizer vocabulary size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvectorizer_vocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match model input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_reg_input_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsure the vectorizer and models are from the same training pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultinomialNB' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "# Load the vectorizer and model\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "log_reg_model = joblib.load(\"log_reg_model.pkl\")\n",
    "\n",
    "# Verify compatibility\n",
    "vectorizer_vocab_size = len(vectorizer.get_feature_names_out())\n",
    "log_reg_input_size = log_reg_model.coef_.shape[1]\n",
    "\n",
    "if vectorizer_vocab_size != log_reg_input_size:\n",
    "    raise ValueError(\n",
    "        f\"Vectorizer vocabulary size ({vectorizer_vocab_size}) does not match model input size ({log_reg_input_size}). \"\n",
    "        \"Ensure the vectorizer and models are from the same training pipeline.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1c4fbede-e968-4bd7-9a60-62cd212e3e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a477b62-2dcb-435b-97e1-6c19f82ef597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nb_model.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(log_reg_model, 'log_reg_model.pkl')\n",
    "joblib.dump(gb_model, 'gb_model.pkl')\n",
    "joblib.dump(rf_model, 'rf_model.pkl')\n",
    "joblib.dump(nb_model, 'nb_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e8de40b-64c1-4267-a09f-604294267645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: newspaper3k in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.9 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flask) (1.9.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (4.12.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (10.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (6.0.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (5.3.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (2.32.3)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (5.1.3)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2024.7.4)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\prash\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.16.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask joblib newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6000f3a3-42ad-46b2-9500-f9099697e46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twenty years on: 'My boat was metres from the shore when the tsunami hit'\n",
      "\n",
      "BBC Just as we pulled out from the harbour, our boat lurched and suddenly the jetty next to where we had boarded collapsed into the sea\n",
      "\n",
      "Boxing Day, 2004. When the earthquake struck at 06:30 (01:00 GMT), I was on a ferry, headed towards Havelock – an island in the Indian archipelago of Andaman and Nicobar. Known for its silver sand and clear blue waters, the Radhanagar beach there had recently been crowned \"Asia's Best Beach\" by Time magazine. My best friend from college and her family had lived in Port Blair, the capital of the archipelago, for a decade and a half, but this was my first visit to the islands, where I had arrived on Christmas Eve. We had planned to spend three days in Havelock and in the morning we packed snacks and sandwiches, gathered excited children and headed out to catch the ferry from Phoenix Bay jetty in Port Blair. Not wanting to miss out on anything, I was standing on the front deck, looking around, when disaster struck. Just as we pulled out from the harbour, the boat lurched and suddenly the jetty next to where we had boarded crumpled and fell into the sea. It was followed by the watchtower and an electricity pole. It was an extraordinary sight. Dozens of people standing alongside me watched open-mouthed. Thankfully, the jetty was deserted at the time so there were no casualties. A boat was due to leave from there in half an hour but the travellers were yet to arrive.\n",
      "\n",
      "Getty Images The tsunami inundated a large number of homes in low-lying areas\n",
      "\n",
      "A member of the boat's crew told me it was an earthquake. At the time I didn't know, but the 9.1 magnitude quake was the third most powerful ever recorded in the world - and remains the biggest and most destructive in Asia. Occurring off the coast of northwest Sumatra under the Indian Ocean, it unleashed a devastating tsunami that killed an estimated 228,000 people across more than a dozen countries and caused massive damage in Indonesia, Sri Lanka, India, Maldives and Thailand. The Andaman and Nicobar islands, located just about 100km north of the epicentre, suffered extensive damage when a wall of water, as high as 15 metres (49 ft) in places, hit land just about 15 minutes later. The official death toll was put at 1,310 – but with more than 5,600 people missing and presumed dead, it's believed that more than 7,000 islanders perished. While on the boat, however, we were oblivious to the scale of destruction around us. Our mobile phones didn't work on the water and we only got snippets of information from the crew. We heard about damage in Sri Lanka, Thailand and Maldives – and the southern Indian coastal town of Nagapattinam.\n",
      "\n",
      "AFP Indian men stand exhausted after searching for missing relatives in Cuddalore, 27 December 2004\n",
      "\n",
      "But there was no information about Andaman and Nicobar - a collection of hundreds of islands scattered around in the Bay of Bengal, located about 1,500km (915 miles) east of India's mainland. Only 38 of them were inhabited. They were home to 400,000 people, including six hunter-gatherer groups who had lived isolated from the outside world for thousands of years. The only way to get to the islands was by ferries but, as we later learnt, an estimated 94% of the jetties in the region were damaged. That was also the reason why, on 26 December 2004, we never made it to Havelock. The jetty there was damaged and under water, we were told. Watch Geeta Pandey speak about her experience here So the boat turned around and started on its return journey. For a while, there was speculation that we might not get clearance to dock at Port Blair for safety reasons and might have to spend the night at anchor. This made the passengers – most of them tourists looking forward to sun and sand – anxious.\n",
      "\n",
      "After several hours of bobbing along in rough seas, we returned to Port Blair. Because Phoenix Bay had been closed following the morning's damage, we were taken to Chatham, another harbour in Port Blair. The jetty where we were dropped had huge, gaping holes in places. The signs of devastation were all around us as we headed home – buildings had turned into rubble, small upturned boats sat in the middle of the streets and roads had great gashes in them. Thousands of people had been turned homeless when the tidal wave flooded their homes in low-lying areas. I met a traumatised nine-year-old girl whose house was filled with water and she told me she had nearly drowned. A woman told me she had lost her entire life's possessions in the blink of an eye.\n",
      "\n",
      "Getty Images In Port Blair, buildings had turned into rubble, small upturned boats sat in the middle of the streets and roads had huge gashes\n",
      "\n",
      "Over the next three weeks, I reported extensively on the disaster and its effects on the population. It was the first time a tsunami had wreaked such havoc in the Andaman and Nicobar Islands and the scale of the tragedy was overwhelming. Salt water contaminated many sources of fresh water and destroyed large tracts of arable land. Getting vital supplies into the islands was tough with jetties unserviceable. The authorities mounted a huge relief and rescue effort. The army, navy and air force were deployed, but it took days before they could get to all the islands. Every day, navy and coast guard ships brought boatloads of people made homeless by the tsunami from other islands to Port Blair where schools and government buildings were turned into temporary shelters. They brought stories of devastation in their homelands. Many told me they had escaped with nothing but the clothes on their backs. One woman from Car Nicobar told me that when the earthquake struck, the ground started to spew foamy water at the same time as the waves came in from the sea. She and hundreds of others from her village had waited for rescuers without food or water for 48 hours. She said it was a \"miracle\" that she and her 20-day-old baby had survived. Port Blair was almost daily jolted by aftershocks, some of them strong enough to start rumours of fresh tsunamis, making scared people run to get to higher ground.\n",
      "\n",
      "Getty Images Thousands of people were left homeless\n",
      "\n",
      "A few days later, the Indian military flew journalists to Car Nicobar, a flat fertile island known for its enchanting beaches and also home to a large Indian air force colony. The killer tsunami had completely flattened the base. The water rose by 12 metres here and as most people slept, the ground was pulled away from under their feet. A hundred people died here. More than half were air force officers and their families. We visited Malacca and Kaakan villages on the island which also bore the brunt of nature's fury, forcing residents to take shelter in tents along the road. Among them were families torn apart by the tidal wave. A grief-stricken young couple told me they had managed to save their five-month-old baby, but their other children, aged seven and 12, were washed away. Surrounded by coconut palms on all sides, every house had turned into rubble. Among the personal belongings strewn about were clothes, textbooks, a child's shoe and a music keyboard. The only thing that stood - surprisingly intact - was a bust of the father of the Indian nation, Mahatma Gandhi, at a traffic roundabout.\n",
      "\n",
      "Getty Images The Indian air force base in Car Nicobar was flattened by the tidal wave\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "url = \"https://www.bbc.com/news/articles/c6230646435o\"\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()\n",
    "print(article.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d82ec9ef-a6e2-4781-a110-e00beba42a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text in Jupyter: Twenty years on: 'My boat was metres from the shore when the tsunami hit'\n",
      "\n",
      "BBC Just as we pulled out from the harbour, our boat lurched and suddenly the jetty next to where we had boarded collapsed into the sea\n",
      "\n",
      "Boxing Day, 2004. When the earthquake struck at 06:30 (01:00 GMT), I was on a ferry, headed towards Havelock – an island in the Indian archipelago of Andaman and Nicobar. Known for its silver sand and clear blue waters, the Radhanagar beach there had recently been crowned \"Asia's Best Beach\" by Time magazine. My best friend from college and her family had lived in Port Blair, the capital of the archipelago, for a decade and a half, but this was my first visit to the islands, where I had arrived on Christmas Eve. We had planned to spend three days in Havelock and in the morning we packed snacks and sandwiches, gathered excited children and headed out to catch the ferry from Phoenix Bay jetty in Port Blair. Not wanting to miss out on anything, I was standing on the front deck, looking around, when disaster struck. Just as we pulled out from the harbour, the boat lurched and suddenly the jetty next to where we had boarded crumpled and fell into the sea. It was followed by the watchtower and an electricity pole. It was an extraordinary sight. Dozens of people standing alongside me watched open-mouthed. Thankfully, the jetty was deserted at the time so there were no casualties. A boat was due to leave from there in half an hour but the travellers were yet to arrive.\n",
      "\n",
      "Getty Images The tsunami inundated a large number of homes in low-lying areas\n",
      "\n",
      "A member of the boat's crew told me it was an earthquake. At the time I didn't know, but the 9.1 magnitude quake was the third most powerful ever recorded in the world - and remains the biggest and most destructive in Asia. Occurring off the coast of northwest Sumatra under the Indian Ocean, it unleashed a devastating tsunami that killed an estimated 228,000 people across more than a dozen countries and caused massive damage in Indonesia, Sri Lanka, India, Maldives and Thailand. The Andaman and Nicobar islands, located just about 100km north of the epicentre, suffered extensive damage when a wall of water, as high as 15 metres (49 ft) in places, hit land just about 15 minutes later. The official death toll was put at 1,310 – but with more than 5,600 people missing and presumed dead, it's believed that more than 7,000 islanders perished. While on the boat, however, we were oblivious to the scale of destruction around us. Our mobile phones didn't work on the water and we only got snippets of information from the crew. We heard about damage in Sri Lanka, Thailand and Maldives – and the southern Indian coastal town of Nagapattinam.\n",
      "\n",
      "AFP Indian men stand exhausted after searching for missing relatives in Cuddalore, 27 December 2004\n",
      "\n",
      "But there was no information about Andaman and Nicobar - a collection of hundreds of islands scattered around in the Bay of Bengal, located about 1,500km (915 miles) east of India's mainland. Only 38 of them were inhabited. They were home to 400,000 people, including six hunter-gatherer groups who had lived isolated from the outside world for thousands of years. The only way to get to the islands was by ferries but, as we later learnt, an estimated 94% of the jetties in the region were damaged. That was also the reason why, on 26 December 2004, we never made it to Havelock. The jetty there was damaged and under water, we were told. Watch Geeta Pandey speak about her experience here So the boat turned around and started on its return journey. For a while, there was speculation that we might not get clearance to dock at Port Blair for safety reasons and might have to spend the night at anchor. This made the passengers – most of them tourists looking forward to sun and sand – anxious.\n",
      "\n",
      "After several hours of bobbing along in rough seas, we returned to Port Blair. Because Phoenix Bay had been closed following the morning's damage, we were taken to Chatham, another harbour in Port Blair. The jetty where we were dropped had huge, gaping holes in places. The signs of devastation were all around us as we headed home – buildings had turned into rubble, small upturned boats sat in the middle of the streets and roads had great gashes in them. Thousands of people had been turned homeless when the tidal wave flooded their homes in low-lying areas. I met a traumatised nine-year-old girl whose house was filled with water and she told me she had nearly drowned. A woman told me she had lost her entire life's possessions in the blink of an eye.\n",
      "\n",
      "Getty Images In Port Blair, buildings had turned into rubble, small upturned boats sat in the middle of the streets and roads had huge gashes\n",
      "\n",
      "Over the next three weeks, I reported extensively on the disaster and its effects on the population. It was the first time a tsunami had wreaked such havoc in the Andaman and Nicobar Islands and the scale of the tragedy was overwhelming. Salt water contaminated many sources of fresh water and destroyed large tracts of arable land. Getting vital supplies into the islands was tough with jetties unserviceable. The authorities mounted a huge relief and rescue effort. The army, navy and air force were deployed, but it took days before they could get to all the islands. Every day, navy and coast guard ships brought boatloads of people made homeless by the tsunami from other islands to Port Blair where schools and government buildings were turned into temporary shelters. They brought stories of devastation in their homelands. Many told me they had escaped with nothing but the clothes on their backs. One woman from Car Nicobar told me that when the earthquake struck, the ground started to spew foamy water at the same time as the waves came in from the sea. She and hundreds of others from her village had waited for rescuers without food or water for 48 hours. She said it was a \"miracle\" that she and her 20-day-old baby had survived. Port Blair was almost daily jolted by aftershocks, some of them strong enough to start rumours of fresh tsunamis, making scared people run to get to higher ground.\n",
      "\n",
      "Getty Images Thousands of people were left homeless\n",
      "\n",
      "A few days later, the Indian military flew journalists to Car Nicobar, a flat fertile island known for its enchanting beaches and also home to a large Indian air force colony. The killer tsunami had completely flattened the base. The water rose by 12 metres here and as most people slept, the ground was pulled away from under their feet. A hundred people died here. More than half were air force officers and their families. We visited Malacca and Kaakan villages on the island which also bore the brunt of nature's fury, forcing residents to take shelter in tents along the road. Among them were families torn apart by the tidal wave. A grief-stricken young couple told me they had managed to save their five-month-old baby, but their other children, aged seven and 12, were washed away. Surrounded by coconut palms on all sides, every house had turned into rubble. Among the personal belongings strewn about were clothes, textbooks, a child's shoe and a music keyboard. The only thing that stood - surprisingly intact - was a bust of the father of the Indian nation, Mahatma Gandhi, at a traffic roundabout.\n",
      "\n",
      "Getty Images The Indian air force base in Car Nicobar was flattened by the tidal wave\n"
     ]
    }
   ],
   "source": [
    "print(f\"Extracted text in Jupyter: {article_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e7bc9f-0ae8-490c-b0fd-994de1e0d074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized text shape in Jupyter: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vectorized text shape in Jupyter: {new_text_vectorized.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e9e0aad-84db-4f91-86c7-23b421d0c39d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel probabilities in Jupyter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfinal_predictions\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Model probabilities in Jupyter: {final_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30e8659f-a3e8-405b-a9eb-c5e8e7e22590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n"
     ]
    }
   ],
   "source": [
    "print(type(log_reg_model))  # This will tell you what type of model is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ebf45-b802-4c19-9b16-42fc2bc6d56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
